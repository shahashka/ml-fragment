{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/cpu:0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_keras(y_true, y_pred):\n",
    "    from tensorflow.keras import backend as K\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "def load_data(num_samples=10000, is3d=False):\n",
    "    arr = pickle.load(open(\"data_large_fixed.pkl\",'rb'))\n",
    "    scores = \"adrp_adpr_A_cat_sorted.csv\"\n",
    "    scores = pd.read_csv(scores)\n",
    "    scores = scores['Chemgauss4'].iloc[0:num_samples].to_numpy()\n",
    "    if is3d:\n",
    "        images_3d = [np.concatenate((i,i[:,:,1].reshape(64,64,1)),axis=2) for i in arr[0][0:num_samples]]\n",
    "        images = np.stack(images_3d,axis=0)\n",
    "    else:\n",
    "        images = np.stack(arr[0][0:num_samples],axis=0)\n",
    "    scaler = MinMaxScaler()\n",
    "    scores = np.abs(scores)\n",
    "    scaled_scores = scaler.fit_transform(scores.reshape(-1,1))\n",
    "    return images, scaled_scores\n",
    "\n",
    "def create_large_model():\n",
    "    with strategy.scope():\n",
    "            base_model = tf.keras.applications.ResNet101( weights=\"imagenet\",\n",
    "                                        input_shape= (64,64,3), include_top=False)\n",
    "            global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "            prediction_layer = tf.keras.layers.Dense(1)\n",
    "            model = tf.keras.Sequential([\n",
    "                base_model,\n",
    "                global_average_layer,\n",
    "                prediction_layer\n",
    "            ])\n",
    "            model.compile(loss=tf.keras.losses.mean_squared_error,\n",
    "                         optimizer=tf.keras.optimizers.Adam(lr=1e-4),\n",
    "                      metrics=['mean_squared_error', r2_keras])\n",
    "    return model\n",
    "\n",
    "def create_small_model():\n",
    "    #with strategy.scope():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(8, kernel_size=(6, 6),\n",
    "                     activation='relu',\n",
    "                     input_shape=(64,64,2),strides=2))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', strides=2))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(8, (3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.mean_squared_error,\n",
    "                  optimizer=tf.keras.optimizers.SGD(lr=1e-3),\n",
    "                  metrics=['mean_squared_error', r2_keras])\n",
    "    return model\n",
    "\n",
    "def prepare_for_training(X,y, cache=True, shuffle_buffer_size=1000, batch_size=64):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    \n",
    "    def generator():\n",
    "        for i, j in zip(X, y):\n",
    "            yield i, j\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(generator, (tf.float32, tf.float32), \n",
    "                                        output_shapes=(tf.TensorShape((64, 64, 3)), tf.TensorShape((1, ))))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "def train(images, scores, isSmall=True, batch_size=64, num_epochs=25):\n",
    "    X_train,X_test, y_train, y_test = train_test_split(images, scores, test_size=0.2, shuffle=True)\n",
    "    model = create_small_model() if isSmall else create_large_model()\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=num_epochs,\n",
    "                        verbose=1, batch_size=batch_size)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 64, 64, 2) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "image, scores = load_data(num_samples=10000,is3d=False)\n",
    "print(image.shape, scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train(image,scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['r2_keras'][1:])\n",
    "plt.plot(history.history['val_r2_keras'][1:])\n",
    "plt.title('Model r2')\n",
    "plt.ylabel('r2')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'][1:])\n",
    "plt.plot(history.history['val_loss'][1:])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "y_test_inv = scaler.inverse_transform(y_test.reshape(-1,1))\n",
    "print(r2_score(y_test_inv, y_pred_inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "print(pearsonr(y_test_inv.flatten(),y_pred_inv.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
