{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from apex import amp\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import glob\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "from dgl import function as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with basic built in GraphConv layers \n",
    "# Define an adjacency matrix using some cutoff threshold\n",
    "# The node features are one hot encoded for protein/ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDistConv(GraphConv):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 norm='both',\n",
    "                 weight=True,\n",
    "                 bias=True,\n",
    "                 activation=None):\n",
    "        super(GraphDistConv, self).__init__(in_feats,out_feats,norm,weight,bias,activation)\n",
    "        \n",
    "    def forward(self, graph, feat, dist, weight=None):\n",
    "        graph = graph.local_var()\n",
    "\n",
    "        if self._norm == 'both':\n",
    "            degs = graph.out_degrees().to(feat.device).float().clamp(min=1)\n",
    "            norm = torch.pow(degs, -0.5)\n",
    "            shp = norm.shape + (1,) * (feat.dim() - 1)\n",
    "            norm = torch.reshape(norm, shp)\n",
    "            feat = feat * norm\n",
    "\n",
    "        if weight is not None:\n",
    "            if self.weight is not None:\n",
    "                raise DGLError('External weight is provided while at the same time the'\n",
    "                               ' module has defined its own weight parameter. Please'\n",
    "                               ' create the module with flag weight=False.')\n",
    "        else:\n",
    "            weight = self.weight\n",
    "\n",
    "        if self._in_feats > self._out_feats:\n",
    "            # mult W first to reduce the feature size for aggregation.\n",
    "            if weight is not None:\n",
    "                feat = torch.matmul(feat, weight)\n",
    "            graph.srcdata['h'] = feat\n",
    "            graph.edata['w'] = dist\n",
    "            graph.update_all(fn.u_mul_e('h', 'w', out='m'), fn.sum('m', 'h'))\n",
    "#             graph.update_all(fn.copy_src(src='h', out='m'),\n",
    "#                              fn.sum(msg='m', out='h'))\n",
    "            rst = graph.dstdata['h']\n",
    "        else:\n",
    "            # aggregate first then mult W\n",
    "            graph.srcdata['h'] = feat\n",
    "            graph.edata['w'] = dist\n",
    "            graph.update_all(fn.u_mul_e('h', 'w', out='m'), fn.sum('m', 'h'))\n",
    "#             graph.update_all(fn.copy_src(src='h', out='m'),\n",
    "#                              fn.sum(msg='m', out='h'))\n",
    "            rst = graph.dstdata['h']\n",
    "            if weight is not None:\n",
    "                rst = torch.matmul(rst, weight)\n",
    "\n",
    "        if self._norm != 'none':\n",
    "            degs = graph.in_degrees().to(feat.device).float().clamp(min=1)\n",
    "            if self._norm == 'both':\n",
    "                norm = torch.pow(degs, -0.5)\n",
    "            else:\n",
    "                norm = 1.0 / degs\n",
    "            shp = norm.shape + (1,) * (feat.dim() - 1)\n",
    "            norm = torch.reshape(norm, shp)\n",
    "            rst = rst * norm\n",
    "\n",
    "        if self.bias is not None:\n",
    "            rst = rst + self.bias\n",
    "\n",
    "        if self._activation is not None:\n",
    "            rst = self._activation(rst)\n",
    "\n",
    "        return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, g, in_feats, h1, h2):\n",
    "        super(GCN, self).__init__()\n",
    "        self.g = g\n",
    "        self.conv1 = GraphDistConv(in_feats, h1)\n",
    "        self.conv2 = GraphDistConv(h1, h2)\n",
    "        self.linear = nn.Linear(h2,1)\n",
    "\n",
    "    def forward(self, inputs, dist):\n",
    "        h = self.conv1(self.g, inputs, dist)\n",
    "        h = torch.relu(h)\n",
    "        h = self.conv2(self.g, h, dist)\n",
    "        h = torch.relu(h)\n",
    "        fp = torch.mean(h, dim=0)\n",
    "        out = self.linear(fp)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, graph):\n",
    "        'Initialization'\n",
    "        self.graph = graph\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.graph)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        return self.graph[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph dataset is the same as image except no interpolation and only one \"channel\"\n",
    "graphs,docks = pickle.load(open(\"dset_jak2_8a_80x80_NEAREST_graph.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein = '#00FFFF'\n",
    "ligand = '#FF00FF' \n",
    "def cutoff(val):\n",
    "    return 1 if val < 0.3 else 0 #only within 3 angstrom considered connection\n",
    "\n",
    "def build_graph(graph,feat, dock):\n",
    "    # read matrix, cutoff at 3 angstroms to create adjacency matrix\n",
    "    adj_matrix = np.array([[cutoff(y) for y in x] for x in graph])\n",
    "\n",
    "    # use networkx to create a graph from adjacency matrix\n",
    "    # visualize\n",
    "    vis = nx.from_numpy_matrix(adj_matrix)\n",
    "    # Kamada-Kawaii layout usually looks pretty for arbitrary graphs\n",
    "    #pos = nx.kamada_kawai_layout(vis)\n",
    "    #nx.draw(vis, pos, with_labels=True, node_color=[ligand if i[0] == 1 else protein for i in feat])\n",
    "    \n",
    "    \n",
    "    # convert to DGL graph\n",
    "    g = dgl.DGLGraph()\n",
    "    g = dgl.from_networkx(vis)\n",
    "    edges = graph*adj_matrix\n",
    "    for i in range(edges.shape[0]):\n",
    "        edges[i][i] = 1\n",
    "    edges=edges[edges!=0]\n",
    "    edges = 1/edges\n",
    "    edges[edges==1]=0\n",
    "    \n",
    "#     # create node features\n",
    "    g.ndata['feat'] = torch.FloatTensor(np.array(feat))\n",
    "    g.ndata['label'] = torch.FloatTensor(dock*np.ones(adj_matrix.shape[0]))\n",
    "    g.edata['inv_dist'] = torch.FloatTensor(edges)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "y = np.abs(docks)\n",
    "for i in range(len(graphs)):\n",
    "    data.append(build_graph(graphs[i][0], graphs[i][1], y[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, _, _ = train_test_split(data,data,test_size=0.2, shuffle=True)\n",
    "train_dataset = GraphDataset(x_train)\n",
    "test_dataset = GraphDataset(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# function to concatenate/batch graphs\n",
    "# creates one large graph per batch that has disjoint subgraphs\n",
    "def collate(data):\n",
    "    graph = dgl.batch(data)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate)\n",
    "g = train_dataset[0]\n",
    "n_classes = 1\n",
    "num_feats = g.ndata['feat'].shape[1]\n",
    "g = g.int().to(device)\n",
    "# define the model\n",
    "model = GCN(g,\n",
    "            num_feats,\n",
    "            8,\n",
    "            8)\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "model = model.to(device)\n",
    "loss_fcn = nn.MSELoss()\n",
    "num_epochs=50\n",
    "loss_train_store=[]\n",
    "loss_test_store=[]\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loss_acc=0\n",
    "    iters=0\n",
    "    for batch, subgraph in enumerate(train_dataloader):\n",
    "        subgraph = subgraph.to(device)\n",
    "\n",
    "        model.g = subgraph\n",
    "        y_pred = model(subgraph.ndata['feat'].float(), subgraph.edata['inv_dist'].float())\n",
    "        loss = loss_fcn(y_pred[0], subgraph.ndata['label'].float()[0])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_acc+=loss.item()\n",
    "        iters+=1\n",
    "    loss_train_store.append(loss_acc/iters)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_acc=0\n",
    "        iters=0\n",
    "        y_pred_values=[]\n",
    "        y_test_values=[]\n",
    "        for batch, subgraph in enumerate(test_dataloader):\n",
    "            subgraph = subgraph.to(device)\n",
    "            model.g = subgraph\n",
    "            y_pred = model(subgraph.ndata['feat'].float(), subgraph.edata['inv_dist'].float())\n",
    "            loss = loss_fcn(y_pred[0],subgraph.ndata['label'].float()[0])        \n",
    "\n",
    "            loss_acc+=loss.item()\n",
    "            iters+=1\n",
    "\n",
    "            y_pred_values.append(y_pred.cpu()[0])\n",
    "            y_test_values.append(subgraph.ndata['label'].float().cpu()[0])\n",
    "#         y_pred_values = [item for sublist in y_pred_values for item in sublist]    \n",
    "#         y_test_values = [item for sublist in y_test_values for item in sublist]    \n",
    "\n",
    "        r2_epoch = r2_score( y_test_values, y_pred_values)    \n",
    "        loss_test_store.append(loss_acc/iters)\n",
    "        \n",
    "    print(epoch, loss_train_store[-1], loss_test_store[-1], r2_epoch)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_train_store, label='train loss')\n",
    "plt.plot(loss_test_store, label='test loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_test_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test_values, label=\"test\", alpha=0.5)\n",
    "plt.hist(y_pred_values, label=\"pred\", alpha=0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.array(y_test_values), np.array(y_pred_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, confusion_matrix, accuracy_score, precision_score, recall_score, roc_curve, auc\n",
    "from regression_enrichment_surface import regression_enrichment_surface as rds\n",
    "rds_model = rds.RegressionEnrichmentSurface(percent_min=-3)\n",
    "rds_model.compute(np.array(y_test_values).flatten(), np.array(y_pred_values).flatten(), samples=30)\n",
    "rds_model.plot(save_file=\"rds_on_cell.png\",\n",
    "                   title='Regression Enrichment Surface (Avg over Unique Cells)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then add weighted adjancency matrix...basically hard coded attention without learnable edge weights. \n",
    "# May need to look at GAT tutorial https://discuss.dgl.ai/t/how-to-build-a-weighted-graph-in-dgl/149/2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
